{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-17 18:38:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Résolution de raw.githubusercontent.com (raw.githubusercontent.com)… 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connexion à raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443… connecté.\n",
      "requête HTTP transmise, en attente de la réponse… 200 OK\n",
      "Taille : 1115394 (1,1M) [text/plain]\n",
      "Sauvegarde en : « input.txt.1 »\n",
      "\n",
      "input.txt.1         100%[===================>]   1,06M  --.-KB/s    ds 0,04s   \n",
      "\n",
      "2024-10-17 18:38:36 (26,9 MB/s) — « input.txt.1 » sauvegardé [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n",
      "length of dataset in characters:  8340015\n",
      "length of dataset in characters:  5045716\n",
      "length of dataset in characters:  4543815\n",
      "length of dataset in characters:  4568170\n",
      "length of dataset in characters:  4072341\n"
     ]
    }
   ],
   "source": [
    "def read_file(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print(\"length of dataset in characters: \", len(text))\n",
    "    return text\n",
    "\n",
    "text1 = read_file('input.txt')\n",
    "text2 = read_file('input2.txt')\n",
    "text3 = read_file('input3.txt')\n",
    "text4 = read_file('input4.txt')\n",
    "text6 = read_file('input6.txt')\n",
    "text7 = read_file('input7.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sentences(text):\n",
    "    sentences = re.findall(r'\\d+\\s+\\S+\\s+(.*)', text)\n",
    "    return \"\\n\".join(sentences)\n",
    "\n",
    "text7 = extract_sentences(text6)\n",
    "with open('input7.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/simondharcourt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  4543815\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the Punkt tokenizer data (only need to run once)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_without_references(text):\n",
    "    pattern = r'\\b(?:[a-zA-Zéèçî]+) \\d+(?:, \\d+)?\\b'\n",
    "    references = re.findall(pattern, text)\n",
    "    # Split the text based on the reference pattern\n",
    "    parts = re.split(pattern, text)\n",
    "    # Combine the non-reference parts to get the text without references\n",
    "    text_without_references = ''.join(parts)\n",
    "    text_without_references = text_without_references.strip()\n",
    "    pattern2 = r'^\\d+\\s+'\n",
    "    text_without_references =  re.sub(pattern2, '', text_without_references, flags=re.MULTILINE)\n",
    "    pattern3 = r'^\\d+(?:,\\s\\d+)?\\s'\n",
    "    text_without_references =  re.sub(pattern3, '', text_without_references, flags=re.MULTILINE)\n",
    "    text_without_references = text_without_references.replace(' - ', ' ').replace(' -', '').replace('- ', '')\n",
    "    return text_without_references\n",
    "\n",
    "def remove_repetitive_lines(text):\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        words = line.split()\n",
    "        if len(set(words)) > 1 or len(words) == 0:\n",
    "            filtered_lines.append(line)\n",
    "    return '\\n'.join(filtered_lines)\n",
    "\n",
    "def remove_accents(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "def remove_dots_next_to_numbers(text):\n",
    "    pattern = r'(?<=\\d)?\\.(?=\\d)'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def remove_numbers_near_letters(text):\n",
    "    pattern = r'\\b\\d+\\b'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "def correct_typos(text):\n",
    "    spell = SpellChecker(language='fr')\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    new_text = []\n",
    "    for word in tqdm(words):\n",
    "        corrected_word = spell.correction(word)\n",
    "        new_text.append(corrected_word)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "def gather_sentences(text):\n",
    "    sentences = re.findall(r'[^.!?]+[.!?]', text)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "\n",
    "text_without_references = extract_text_without_references(text3)\n",
    "text_without_references = remove_repetitive_lines(text_without_references)\n",
    "# text_without_references = remove_accents(text_without_references)\n",
    "text_without_references = remove_dots_next_to_numbers(text_without_references)\n",
    "text_without_references = ' '.join(text_without_references.split('\\n '))\n",
    "text_without_references = remove_numbers_near_letters(text_without_references)\n",
    "# text_without_references = correct_typos(text_without_references)\n",
    "\n",
    "sentences = sent_tokenize(text_without_references)\n",
    "sentences = [sentence.strip() for sentence in sentences if len(sentence) > 1]\n",
    "text_without_references = '\\n'.join(sentences)\n",
    "\n",
    "text_without_references = gather_sentences(text_without_references)\n",
    "\n",
    "\n",
    "# Save the modified text to a file\n",
    "with open('input4.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(text_without_references)\n",
    "text4 = read_file('input4.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from translate import Translator\n",
    "\n",
    "# def translate_to_english(text):\n",
    "#     translator = Translator(from_lang='fr', to_lang='en')\n",
    "#     translated_text = translator.translate(text)\n",
    "#     return translated_text\n",
    "\n",
    "# text5 = translate_to_english(text)\n",
    "# with open('input5.txt', 'w', encoding='utf-8') as file:\n",
    "#     file.write(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  4072341\n"
     ]
    }
   ],
   "source": [
    "text = read_file('input7.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 773588\n",
      "Sentence count: 29212\n",
      "Syllable count: 945851\n",
      "Flesch Reading Ease: 78.42\n",
      "Flesch-Kincaid Grade: 8.9\n",
      "Coleman-Liau Index: 6.74\n",
      "Automated Readability Index: 11.9\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "\n",
    "def evaluate_text_complexity(text):\n",
    "    word_count = textstat.lexicon_count(text)\n",
    "    sentence_count = textstat.sentence_count(text)\n",
    "    syllable_count = textstat.syllable_count(text)\n",
    "\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade = textstat.flesch_kincaid_grade(text)\n",
    "    coleman_liau_index = textstat.coleman_liau_index(text)\n",
    "    automated_readability_index = textstat.automated_readability_index(text)\n",
    "\n",
    "    print(\"Word count:\", word_count)\n",
    "    print(\"Sentence count:\", sentence_count)\n",
    "    print(\"Syllable count:\", syllable_count)\n",
    "\n",
    "    print(\"Flesch Reading Ease:\", flesch_reading_ease)\n",
    "    print(\"Flesch-Kincaid Grade:\", flesch_kincaid_grade)\n",
    "    print(\"Coleman-Liau Index:\", coleman_liau_index)\n",
    "    print(\"Automated Readability Index:\", automated_readability_index)\n",
    "\n",
    "evaluate_text_complexity(text7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 794577\n",
      "Sentence count: 38054\n",
      "Syllable count: 1007183\n",
      "Flesch Reading Ease: 75.64\n",
      "Flesch-Kincaid Grade: 7.9\n",
      "Coleman-Liau Index: 8.36\n",
      "Automated Readability Index: 11.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_text_complexity(text4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 202646\n",
      "Sentence count: 11942\n",
      "Syllable count: 253183\n",
      "Flesch Reading Ease: 88.06\n",
      "Flesch-Kincaid Grade: 5.2\n",
      "Coleman-Liau Index: 6.78\n",
      "Automated Readability Index: 8.1\n"
     ]
    }
   ],
   "source": [
    "evaluate_text_complexity(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"'(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVXYZabcdefghijklmnopqrstuvwxyz\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_txt = enc.encode(text)\n",
    "\n",
    "encode = lambda s: enc.encode(s) \n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.810461 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [02:16<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 199\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(max_iters)):\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m     \u001b[39m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m%\u001b[39m eval_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39m==\u001b[39m max_iters \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 199\u001b[0m         losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[1;32m    200\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstep \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: train loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, val loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m     \u001b[39m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[3], line 59\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[1;32m     58\u001b[0m     X, Y \u001b[39m=\u001b[39m get_batch(split)\n\u001b[0;32m---> 59\u001b[0m     logits, loss \u001b[39m=\u001b[39m model(X, Y)\n\u001b[1;32m     60\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 155\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    153\u001b[0m pos_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_embedding_table(torch\u001b[39m.\u001b[39marange(T, device\u001b[39m=\u001b[39mdevice)) \u001b[39m# (T,C)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m x \u001b[39m=\u001b[39m tok_emb \u001b[39m+\u001b[39m pos_emb \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_f(x) \u001b[39m# (B,T,C)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(x) \u001b[39m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 132\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 132\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x))\n\u001b[1;32m    133\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n\u001b[1;32m    134\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 100\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[3], line 100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 100\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(x) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m, in \u001b[0;36mHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m wei \u001b[39m=\u001b[39m q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m C\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m \u001b[39m# (B, T, C) @ (B, C, T) -> (B, T, T)\u001b[39;00m\n\u001b[1;32m     82\u001b[0m wei \u001b[39m=\u001b[39m wei\u001b[39m.\u001b[39mmasked_fill(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtril[:T, :T] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m)) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m wei \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49msoftmax(wei, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# (B, T, T)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m wei \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(wei)\n\u001b[1;32m     85\u001b[0m \u001b[39m# perform the weighted aggregation of the values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py:1858\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1856\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1858\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1859\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1860\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else 'mps'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# # wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# here are all the unique characters that occur in this text\n",
    "text = text4\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tbetter a could in the roid monches, and knownown art his eary burnt was that wings done shall clast, not they tarks of the father oar aradve and all to forsad king: because their ghord cast fouth from accord, and forty dostress, in the father of burder offering? his father Assurely a man, in they priest: becaughtered down wiss years, the king of Isromies, disciple town with in the sire town of windown lifeen?\n",
      "thou the ward do.\n",
      "the away fore monneyparet, and Eleerishan to him again to the LORD wave beat the bird.\n",
      "323:46\tSo from in the names of offering-deek and their hand full with Haramallay; I he, are I but thou not asked to this shiper.\n",
      "brettle; the word, hend is that he maid in against Tyut, offer spake quiZnace for ever come the falted burnt offlocks, with to perfilly that I may the midst of the mention; of rejoice, their shall gra: ackid youtness wates bery for a men people against the forcerfars offerings,\n",
      "of knowest the einhap whence and struve offer the Gegriah words all the eyenemies sun by with to with the mostly; which God not wave, Then westath.\n",
      "that I welletute.\n",
      "Moses,\n",
      "the day not magious to Amon.\n",
      "the trighty hand, and sayptaoughout, Abrahan, and hand peass shall hip hands hears.\n",
      "Josephiples, and bost offfle.\n",
      "17:43\tBut down one evil that I may it down any hundred to thou requeth touth an that year hand and skevesy command Con thes known hing thou him. song which had that were of resseded, and break of the layess of the blather yoen by thereants of the thurd.\n",
      "hond givelligning,\n",
      "in of the inheritiqui I lives a very Serarah. How up onto the bordervan of Isaiah a mighterly, and what their sword, and have bullown. And eet stought by the Lord. Hecam wound, and from the chiefnesses of massehough: not and is set neither save to Judah: the pown of the heart offlesher, and bukned minchied; into the his brake LORD’S; and I seouk there aftery as in Eneileed, rettopren, they say and the heenk I not seed, for asseke save him, Wiskness.\n",
      "said unto Allun the trephron.\n",
      "they make?\n",
      "far is they seluburn and make unto Joh, Ashethal for they may befrue hind wents.\n",
      "it up all Jerusalem, for and men men, uftith seen in to the fators, and over say for all the woodd may with twell not frour apassare of the times; whither rong of Mim, Whether after the LORD and is Gol the hand, and had sername until that king Bat orly: many mend their son shall be be sake unto the child by Israel was gorreatheth.\n",
      "Hezillon togethethat hholy naken if him, and the offly us, withle;\n",
      "God the dwoor ard understanding ancepting to God corn, and the tabernest;\n",
      "they shall ye vouring and goon unto all door in in the things.\n",
      "us my hands for Israel, thou is lin the woodn off Tyrie turnest the bearle to the multitude offor Corn faints.\n",
      "unto thee, it love the roopsn of the Ricear of the chief mine not hath maden thou came and lay was, Larah the Gileons of parts: ye shall went year the overth wast repreder: anger the pross of offering dake your among the heart.\n",
      "hand to made withden eermoth form, monds, which why thou hast by the dead: and aware all yook came, thous lalks that\n",
      "eudge unto I haveding from after him: let that with her were excaptains befors: my whoulderad forth allown him ten, prust the way the thy LORD the wort.\n",
      "risisn us day their yes conding to for all thou hand is ender mentanains, and was fell ressels:\n",
      "thou say, I things and far every redies-alon; thoukeld upon Shimon, if hundred mading endaster Token shall be said unto Jeroban the gold.\n",
      "year far shall ashe brosss.\n",
      "LORD have earden captificisy, and rebin, it for a offering fraid no asseking one radve; for in the Mundernehan ye shall feor joice: and thereof the LORD unto me.\n",
      "12:17\tAnd Hedry inderought from the city wife:\n",
      "21:24\tThou Jewar and Rarothan’s bles, fromer unto us.\n",
      "ye shall sake ye in with the gatethin-isher, and servingvess, even say:\n",
      "it who may reves out of the countily offor even, saying yet.\n",
      "Kither thy arosfe; and day them, Ammade not workn Abinah.\n",
      "hatsoughour four the way.\n",
      "28:19\tThen Hekich, and say deliving high blood; for Sor; and all the wany: for the drumed of lighter’s syether, say4: Where cubbins to you, in the gold, and batter in thou God which hadficken and is send Pererians than of Judah in the feyess spared turn God.\n",
      "12:13\tAnd by coventant shall not dorke of as ould with hand rejereding and made gather to clay.\n",
      "down Doligh hath reIsreed up inderstancies frown faced to mininishence out in the litties; and the first, and dided asserve thee.\n",
      "13:14\tars on the naminess oxter him owch also thought mights, and say the forfoun to offering.\n",
      "cast in there.\n",
      "they miny hand very tolle of mot8: nor shall bring to my offering.\n",
      "Solomah do was vination. The LORD: As the peoplass and they shumber, and into to the ennaminanus, as they jurple didie, bet thee to that the children mong and forther of your, and breaks to Tarathonssomen, even unto the LORD, so know every goods of wived unto they roarroin in the tamfuling.\n",
      "34:5\tHut sennem, the, which is the roughour of the vadstain, notsedged your forcy.\n",
      "it day them Shrethab boin make said by forther and Isaab Israel remings,\n",
      "9:13\tAnd theut among them.\n",
      "21:6\tAnd called come not playn land born knew for six Thiney may hand upon the round to Jorda workn, and the him, not he put thee roign us, and his fay thou wind day these north of huand? The peopleeth shall be latuttee him, and degeot, and give thee, and he willought the camen of their bally offilless which he and that Jeturelem round an: as thund after incerpaxed I will the evicked of the law off end thout, condst I all the brother rehighing leat time by the work gople of ments;\n",
      " hast trubly not look poursedgether, Kish heared with heard all the LORD of host: and your ear, but they not every make farted could every me clean ound offineby and Bashather fwhent, bece sons among manner, and all the brather.\n",
      "26:32\tAnd of Zephanain?\n",
      "the people; comen and hind it.\n",
      "an their sezeth forber, for the woods.\n",
      "of the eyes for him, even and\n",
      "seedgether Zorger hates, saying, Caul oull-hab gather that I all there town wmays said unto morn their gliving.\n",
      "the king incering pleass every bare did indering whevous all awayerneth the king of Beholda eveks; and gater; as thus nother trumbler: yuill saith the Lord to Mystron lawfifulful; went out, behold man and day said unto done.\n",
      "the shember, and as not their preputates to the LORD.\n",
      "tell the mouths, in thou they came for meet, and to give bought thore fast up it; thereof, bokecher afare and lown were things, thould be ladge him tarrown.\n",
      "is drown said unto us.\n",
      "said, He not.\n",
      "that were as now down to house and young curp after they heart that drived.\n",
      "15:3\tAnd onger and cast for his name to unto the your Shumiteth Naham saith twenting that and dwell it be; and the hundue offare Omedan went fround him under.\n",
      "ye.\n",
      "the gentereth ofder work upon yousness of thou sare things the word thou and of the doeth if the ealenth.\n",
      "there, the Jidodah with the servatesse against hathous, who nor not recant numbered beloved with an, with passed I forth whats, the heard.\n",
      "hath was the LORD said unto Aaronditan.\n",
      "17:13\tAnd that Tokeal-el, I of is sillee, that the name offer thee face yound reaturn day raished to thee.\n",
      "Defore, and tockent me? and Asiah, the ears.\n",
      "Myrath; and remuber young ear anding chaint time house thing to stevent turn unto Mazberam, and the wayoung rot.\n",
      "also foutness of say it.\n",
      "sai therefore safver, Sire; he said, Thria snal. Awhoron the! was were down: saye fought off vacle of the LORD.\n",
      "I assarmited all as say into the LORD: to my ven offering.\n",
      "menderaid that Egypty?\n",
      "is the willomenful.\n",
      "rembelly, Callear thy woulnest of the sisnightion of the children, saying, Thou didst of the pertrain offlooUl of Israel, and sceive the citys of Ammarah And to him; thoughour glorrousnest and going, and upon their heart of Leven men.\n",
      "the sountain, are down in this is seon of thee, who well in men, and dind of the us to Ajalah any thefore, whicD deliver voice up for thy Peirmels, and kseek not your: they Shelover made madon evilf.\n",
      "Zuknish heaven table to get; as the vatter a bridder many tounder of for Savi; and may their housions of dran beed ye, and saidged thee ofslaim minst their dreiver; and with right by forther evil offerings, and turn a four dway; it when they sceptives them the is word upon the father; there men, bork hadloites and the earch off Israel.\n",
      "11:37\tAnd he neal nation and hepher us.\n",
      "I lifes of serve incenting, to for Moseremom, A her face rast up with heart.\n",
      "mettle return.\n",
      "the LORD God surely that thou harle after jayity whick for the LORD stillar me:\n",
      "at what playce, every ran stran and the words, and carroter away the morny all Jesusuel, and his eat beaster, and water andanger delay in by assembled’s try Samariel to mime.\n",
      "4:12\tAnd I willd, sayched, End will cames, and said, Winks of Israel, and server.\n",
      "the cornsubber over them: the woughder went destroment, wholen evil, be Syrehadar’s and and LORD, O LORD in at the fiering, that I bur having to minity evon of Madoibs hus, year fast twelve round bindrded for shipch by the sons of the hirly leak; and ageeth thou hand to him among the sonew which ye deletce, parts thy crons top and herd, saying.\n",
      "that yet deliver my bock thing, by goether fubled which fither tring poirtle unto ploint Joseudar.\n",
      "say the LORD, Geld in a tengreder than of the word; of the right thou continuuzber young forsarectains my and no are peofeded.\n",
      "they sirve turn and sead turned with God the dribce; O they cring may cast asse Abur: in was shall the king’s to their Howezer, and the krewell numitee but for him us, ye him, inhaber fillow thous) the Leviter these to strenger, which every are my groed:\n",
      "ong the honqughteen of the house, and withones down with yours his smist be inuelfored it, and dectips, and the morness: and he madeethere before Sanner: and Jeron, assebering that onsear transs? one menders their was cover the high reatines.\n",
      "Zabosh who coin’s affitury angelt the basservies of the Gathant, and itabite to lies, and waded it passon unto the cities for iver; in the earth what say that is your rasains, and all condens and heud fornost cast thou hast her makness, think will they eyes a ineendeaseful. As statutee which were trike come a tookle of the deedling, and be known time.\n",
      "seejek did time in medgound youns, and me in the trengation Rebite, do youl; thy lablour rensered the burneming delivered, and broke intury dour all the south peacople.\n",
      "11:44\tAnd the conds of year assembless regain it thou the diside.\n",
      "if the seed.\n",
      "am for your a godsagnoued say burd hneyoskinfice as all Israel that Aramia, the glor off endured unto shings; I will havow and that is servantips: my was offering.\n",
      "22:8\tAnd Isaa, (or Hamir eyes, thought or a bringer at again, Host which son froar the offery gatuel year light a puldredness: nour from in are city seender said unto Bazar at wuth made, and in him.\n",
      "seed heart with it Moses of Israel, and three, the rolly of Beth-elder from: all the Chimear the hagath of the king of Eguzzzrahol, that they stubur and cast in Jeropham, that passed possesSerites, and the into the kings wardden and treed that are down him, that is day to 9:23\tThou gathe cometh my Gan, and end mucy not lood fwell is thitre upon men.\n",
      "the ways.\n",
      "shepead all treaso time.\n",
      "said sire Untablittone undor from the yough, and Levid thee wing of the fatter is the arken sire, end this day of the LORD thy God unto the wepting the nastress of hast in of thee?\n",
      "Lother hadged togreen offerings, that ye forsay are grupteth the LORD: whos sond thip is thip to lown sain the LORD unto mouther ender San, I shme all say over, yet, Do well nycomure hand toway from it, and burnt say day to the father four of throughen to yours of the blood a nally.\n",
      "who neither hike, and the king’t the cluffless, things afflice, saying be voily doned us it; there heart: and gone thou shall fering up.\n",
      "and Inreen, dwell the LORD the loves shall bare the it thereof’s our foor ever.\n",
      "lifive years serve.\n",
      "flroth foole me, hath rezitter: he shall be good shall countain.\n",
      "the ear caJes; and at thou sweprets into me, and border, and sing the wicked, what said unto Syral\n",
      "thithe son of the childness, in says; even tire, that which was fallom these thee ul amond Carcer me, and passe.\n",
      "they may hand rond hughffers himselfftinness:\n",
      "17\ttheir waide.\n",
      "in undercryption unto thee: and the day regived and the escape off the works of Jacob house, Who sone endared was aboul, the voicist ust.\n",
      "Islaah unto me: thee time, that stempteof frear to done brother:\n",
      "the corning to any own throughPeriah nort bendin;s\n",
      "it have was dwelpaked judIst off these: and Koather men, and have rags;\n",
      "not your shalt it not time;\n",
      "at their righteousney the reming the LORD.\n",
      "18:38\tTo they before thee, and went is wars.\n",
      "the fassempt is Belaken, yet firshbutances and for years made wall’s ender yeder hond, and seven one the rogress, what say so, Peter hent Gevil the neighweouth year warder.\n",
      "thou thenge, and by thoughter; they should serve grown to advidon, do shall set He from a weltiquity eyout, and become to me; they not norgusnely are offer unto the fie, What which deathered save up tinto the father who woudgethatger unto Bor: yet.\n",
      "8:23:26\tHebupheth said, O place madre and forkness, and a prophet offery offlif.\n",
      "2et therefore hort?\n",
      "son that I said withdous, and hOst may for my Ami- sare until, Gol is against their oney known upon heart unto the LORD, evense the men.\n",
      "Jorobarise reign thee, whats he shall be taken, of the grearweth, not even or the shourper sighty:\n",
      "their spake the LORD.\n",
      "and the is covennallings of them’s rorned bretter? of Israel.\n",
      "the LORD great as lodd turn undernother.\n",
      "of Israel, Any servant upon Zodon in the given. And all will gother from hre wine; and goodned (for him in him.\n",
      "17:26\tAnd Ssurel hath is to power thereof, he day.\n",
      "hall they went went whither that waker the righteous and him among as, and the earths of men.\n",
      "Jazzar the sweep of proving to consumblet togdo Mos the terrings, and wings god unto Isra woundephnessedove unto the Aannegar.\n",
      "15:16\tThend shall kne: Wind it that his loo.\n",
      "hath cal you, and you.\n",
      "324:26\tAnd his mord, The daughtead;\n",
      "lown to clay: and of Jordam, or hast mine, with out I quickech to bird.\n",
      "but four compars: for hy say of the mercing’s heard my offery.\n",
      "4:25\tThelike,\n",
      "he said unto the Sleast.\n",
      "13:26\tLORD the departanced and Peters take because my abirition for house;\n",
      "it purt up not the madgres, and the LORD.\n",
      "whose stread me, and Leazzallem, Saut it peace of Haddehon earful sever high that veleant, hoth not dother over his houses to the children sin tains periest thou comnings, though the movany dreather thou detivile be dreath.\n",
      "18:6\tBnd Saul-ar seed not the werke on my rost bear: as whenceise Jarany door of Moses cought to children in ot not hear bans at eide wre, not say.\n",
      "weve lifelting the eenars: O Tan, way dyoung that the young of house.\n",
      "they kneir down not into the allemeth of the fitur.\n",
      "and down in men forging Edomiah in his goove; par which Jetweth the LORD: yet onua your and bound trumptation-endimon to.\n",
      "they Aram ober, is for the floorshem mayserved me men qullor for his debard hand and his south trans are gone in the tambers; of for a nother gotcherathand wells with sead son you father’s hone, even the letdors.\n",
      "are a Highbyla, they young mertining David fallows are to disk mples. What not Zerhalarah; Jeha lijamed made for away hundred a foot and farour brought year: who suburath; of year from all to pass which of men? and faull among the word in mervoophats,\n",
      "they was stoon they walked afre all in the land of Godil.\n",
      "the compass not Meword this chief.\n",
      "the rejoice shall not, people offerings of the Philiee-stard, and the Lword your word; they host five: no the turned of the face reast offtrom that to cipst with were into nice year upon hearched the children offerings to Israel; he cast that thou to put ying,\n",
      "32:37\tAnd the concerning ye:\n",
      "12:8\tand the LORD God hath this word found eact to not gone women?\n",
      "ovet but not him poor all nongent inserved, and rastain.\n",
      "evered to they four for a feirted, and Inem thing forth, and that ut with requiry is the oneyes retur you, And afrom the hou son of God in, and cenceanner the fetdren fay thee.\n",
      "long off is would in high, one dire unto me.\n",
      "any were montinany, which he heart unto Johaphat; and they whose heartdeth of the man off the Laphableon offering;\n",
      "the silvargue the God me any the son of Lidotlahes.\n",
      "the bown my taber.\n",
      "when sacrifice, Wodlk Jerehosh, thou wornd year the passon all gevour that Elishon, and daughter the place.\n",
      "against thoughout a loanem offling,\n",
      "lette nonceived not shalleed, or the priests the son of Meneceit God of to parters.\n",
      "in iffyour incerlain, and heaketh is not; fles not hearl did Hallar, in the taegether stout, Wake not which was tooke way.\n",
      "29:27\tNof is eving.\n",
      "my way inglethered.\n",
      "if thing the LORD, O LORD, do father joinger that parts him wast him, and Pabyriance tooketh offering the noney.\n",
      "21:16\tAnd shall be porsargings.\n",
      "18:12\tAnd the hofge, and I will door for ever.\n",
      "mutth did sass that!\n",
      "say unto me.\n",
      "the poorssewed unto Alasseer him, from whence was notvor, then shall be the seek have sire trewbly and it were them off unto all Israeth cast in the inderlare, and Simon, and.\n",
      "he bur sixtion evil.\n",
      "Merose notedon, and they my brenger yet a waith; of Ishakle of his perferings.\n",
      "72:13\tWhen that is comndand, and I who dread prospery take a gave conder into things the last off braing, and nother the nations, notne that had soung your sons of o whoreings; and the father, as thips not forgar souberness a, and man written in to fall-cinger.\n",
      "the faishshinamand of with man house unto heepr, O kneth sintucuity away.\n",
      "thou thee myndresh I am not for without fight.\n",
      "the LORD made what not is clorgre unto him, (noar hundred in the king, onfor it: they not to the father of be herreds the journeyed about, and the Lord.\n",
      "yesh serve abountain, and have to father which hould evil, and the neebors.\n",
      "2Mon the seek him, and Jehobocapher to the from upon hings:\n",
      "they had bround metter and house for your tyethre fiether ofar against in the first of by the wardong, and hear your Judah the LORD, Catter unto Agaron thy purppe it again. Habylon.\n",
      "31,ep for men is not, and cannublessed out thee.\n",
      "it way! be dieds, and give thee, and, the word of two brake a forrancasyes.\n",
      "their and treass that God, and yend the people, saying Sirahan yet graoke yours, and Ashut coven to firs.\n",
      "32:16\teven made and out the king of the roncheturpitar stayeng Jordan.\n",
      "9:10\tBendon day put to thee.\n",
      "the LORD: there spake by rescasfied flam for yurning me.\n",
      "are by the Hakifes signer, and bingeth the child.\n",
      "ear.\n",
      "54:16\tToke after Samon ye, his word:\n",
      "yet have before mine: and Pecretionse forsave him tritte.\n",
      "years no froor fell dreafy gathered him, gather to their suby to the work of the son; and many basse forsament heard the day of withdouss than thy give face gword, and coud of the king’s herchedp and nothing sidee.\n",
      "it smist turnesse of hreed not taketh on the men up nor the oarkness, which out him, he wecume as in the wrays on the people not maystrancheding rong pitcle found in hinguah in hime hand man the bulfferled: and the goon lifkon of the work, the seep in the Ashami, and cut the outher of theirty took the youngers of his hand of Abylah-gaterner hall you safy ye mornings: and is men it to chambar’s.\n",
      "and Hadeve, and feer and way.\n",
      "The his said unto Alioul to the Asnam, the LORD is?\n",
      "hald they ender Jorabobli.\n",
      "8:43\tFor hown how us the was round bear to own; mine the reigrain unto the power in plasse mervan.\n",
      "the enter honour.\n",
      "and lay serve altaw and the wood in a traves ot two kedled.\n",
      "it will far which they toleeth to pass, there may lotted go righty.\n",
      "it is to the grewed heart, and all the lagainst not hall of the shall gath abounter hand which ye inhering in in house every his sward whosood, and hafter the day of the son of Shiromein the arketh towell what the younding perils seep, We on year.\n",
      "13:16\tAnd the fulfilly offering Israel.\n",
      "whicL shall remy nake them in the came not. Assrervaing, A cast young, art the LORD w\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
