{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorties: [[[0.49991064]\n",
      "  [0.49997851]]\n",
      "\n",
      " [[0.50008478]\n",
      "  [0.50000366]]\n",
      "\n",
      " [[0.50001331]\n",
      "  [0.50002984]]\n",
      "\n",
      " [[0.4998113 ]\n",
      "  [0.50012126]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, taille_entree, taille_cachee, taille_sortie):\n",
    "        self.taille_entree = taille_entree\n",
    "        self.taille_cachee = taille_cachee\n",
    "        self.taille_sortie = taille_sortie\n",
    "        \n",
    "        # Initialisation des poids\n",
    "        self.W_xh = np.random.randn(taille_cachee, taille_entree) * 0.01\n",
    "        self.W_hh = np.random.randn(taille_cachee, taille_cachee) * 0.01\n",
    "        self.W_hy = np.random.randn(taille_sortie, taille_cachee) * 0.01\n",
    "        \n",
    "        # Initialisation des biais\n",
    "        self.b_h = np.zeros((taille_cachee, 1))\n",
    "        self.b_y = np.zeros((taille_sortie, 1))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def etape_rnn(self, x, h_prev):\n",
    "        # Calcul de l'état caché\n",
    "        h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h_prev) + self.b_h)\n",
    "        # Calcul de la sortie\n",
    "        y = self.sigmoid(np.dot(self.W_hy, h) + self.b_y)\n",
    "        return h, y\n",
    "    \n",
    "    def propagation_avant(self, X):\n",
    "        h = np.zeros((self.taille_cachee, 1))\n",
    "        sorties = []\n",
    "        \n",
    "        for x in X:\n",
    "            h, y = self.etape_rnn(x, h)\n",
    "            sorties.append(y)\n",
    "        \n",
    "        return np.array(sorties)\n",
    "\n",
    "\n",
    "\n",
    "    def propagation_arriere(self, X, Y, sorties, taux_apprentissage):\n",
    "        dW_xh = np.zeros_like(self.W_xh)\n",
    "        dW_hh = np.zeros_like(self.W_hh)\n",
    "        dW_hy = np.zeros_like(self.W_hy)\n",
    "        db_h = np.zeros_like(self.b_h)\n",
    "        db_y = np.zeros_like(self.b_y)\n",
    "\n",
    "        h = np.zeros((self.taille_cachee, 1))\n",
    "        dh_next = np.zeros_like(h)\n",
    "        \n",
    "        for t in reversed(range(len(X))):\n",
    "            dy = sorties[t] - Y[t]\n",
    "            dW_hy += np.dot(dy, h.T)\n",
    "            db_y += dy\n",
    "            \n",
    "            dh = np.dot(self.W_hy.T, dy) + dh_next\n",
    "            dh_raw = (1 - h**2) * dh\n",
    "            db_h += dh_raw\n",
    "            dW_xh += np.dot(dh_raw, X[t].T)\n",
    "            dW_hh += np.dot(dh_raw, h.T)\n",
    "            \n",
    "            dh_next = np.dot(self.W_hh.T, dh_raw)\n",
    "            \n",
    "            h, _ = self.etape_rnn(X[t], h)\n",
    "\n",
    "        # Mise à jour des poids et biais\n",
    "        self.W_xh -= taux_apprentissage * dW_xh\n",
    "        self.W_hh -= taux_apprentissage * dW_hh\n",
    "        self.W_hy -= taux_apprentissage * dW_hy\n",
    "        self.b_h -= taux_apprentissage * db_h\n",
    "        self.b_y -= taux_apprentissage * db_y\n",
    "\n",
    "    def entrainer(self, X, Y, epochs, taux_apprentissage):\n",
    "        for epoch in range(epochs):\n",
    "            sorties = self.propagation_avant(X)\n",
    "            self.propagation_arriere(X, Y, sorties, taux_apprentissage)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                perte = np.mean(np.square(sorties - Y))\n",
    "                print(f\"Époque {epoch}, Perte: {perte}\")\n",
    "\n",
    "# Exemple d'utilisation\n",
    "taille_entree = 3\n",
    "taille_cachee = 5\n",
    "taille_sortie = 2\n",
    "rnn = RNN(taille_entree, taille_cachee, taille_sortie)\n",
    "\n",
    "# Données d'entrée (séquence de 4 vecteurs d'entrée)\n",
    "X = np.random.randn(4, taille_entree, 1)\n",
    "\n",
    "# Propagation avant\n",
    "sorties = rnn.propagation_avant(X)\n",
    "print(\"Sorties:\", sorties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme de la sortie: (2, 10, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Transformer:\n",
    "    def __init__(self, taille_vocab, dim_modele, nb_tetes, dim_ff, nb_couches, is_decodeur=False):\n",
    "        self.taille_vocab = taille_vocab\n",
    "        self.dim_modele = dim_modele\n",
    "        self.nb_tetes = nb_tetes\n",
    "        self.dim_ff = dim_ff\n",
    "        self.nb_couches = nb_couches\n",
    "        self.is_decodeur = is_decodeur\n",
    "\n",
    "        \n",
    "        # Initialisation des poids\n",
    "        self.embedding = np.random.randn(taille_vocab, dim_modele)\n",
    "        self.couches = [TransformerCouche(dim_modele, nb_tetes, dim_ff) for _ in range(nb_couches)]\n",
    "        self.fc = np.random.randn(dim_modele, taille_vocab)\n",
    "        self.encodage_position = EncodagePosition(dim_modele)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding[x]\n",
    "        x = self.encodage_position.forward(x)\n",
    "        masque = None\n",
    "        if self.is_decodeur:\n",
    "            masque = self.creer_masque_causal(x.shape[1])\n",
    "        for couche in self.couches:\n",
    "            x = couche.forward(x, masque)\n",
    "        return np.dot(x, self.fc)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class TransformerCouche:\n",
    "    def __init__(self, dim_modele, nb_tetes, dim_ff):\n",
    "        self.attention = AttentionMultiTetes(dim_modele, nb_tetes)\n",
    "        self.ff = FeedForward(dim_modele, dim_ff)\n",
    "        self.norm1 = LayerNorm(dim_modele)\n",
    "        self.norm2 = LayerNorm(dim_modele)\n",
    "        \n",
    "    def forward(self, x, masque=None):\n",
    "        x = x + self.attention.forward(x, masque)\n",
    "        x = self.norm1.forward(x)\n",
    "        x = x + self.ff.forward(x)\n",
    "        return self.norm2.forward(x)\n",
    "\n",
    "class AttentionMultiTetes:\n",
    "    def __init__(self, dim_modele, nb_tetes):\n",
    "        self.nb_tetes = nb_tetes\n",
    "        self.dim_tete = dim_modele // nb_tetes\n",
    "        self.dim_modele = dim_modele\n",
    "        self.wq = np.random.randn(dim_modele, dim_modele)\n",
    "        self.wk = np.random.randn(dim_modele, dim_modele)\n",
    "        self.wv = np.random.randn(dim_modele, dim_modele)\n",
    "        self.wo = np.random.randn(dim_modele, dim_modele)\n",
    "    \n",
    "    def creer_masque_causal(self, size):\n",
    "        return np.triu(np.ones((size, size)), k=1).astype(bool)\n",
    "    \n",
    "        \n",
    "    def forward(self, x, masque=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        q = np.dot(x, self.wq).reshape(batch_size, seq_len, self.nb_tetes, self.dim_tete)\n",
    "        k = np.dot(x, self.wk).reshape(batch_size, seq_len, self.nb_tetes, self.dim_tete)\n",
    "        v = np.dot(x, self.wv).reshape(batch_size, seq_len, self.nb_tetes, self.dim_tete)\n",
    "        \n",
    "        q = q.transpose(0, 2, 1, 3)  # (batch_size, nb_tetes, seq_len, dim_tete)\n",
    "        k = k.transpose(0, 2, 1, 3)\n",
    "        v = v.transpose(0, 2, 1, 3)\n",
    "        \n",
    "        scores = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(self.dim_tete)\n",
    "        if masque is not None:\n",
    "            scores = np.ma.masked_array(scores, mask=masque)\n",
    "        \n",
    "        attention = self.softmax(scores)        \n",
    "        out = np.matmul(attention, v)\n",
    "        out = out.transpose(0, 2, 1, 3).reshape(batch_size, seq_len, self.dim_modele)\n",
    "        return np.dot(out, self.wo)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "class FeedForward:\n",
    "    def __init__(self, dim_modele, dim_ff):\n",
    "        self.w1 = np.random.randn(dim_modele, dim_ff)\n",
    "        self.w2 = np.random.randn(dim_ff, dim_modele)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(self.relu(np.dot(x, self.w1)), self.w2)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "class LayerNorm:\n",
    "    def __init__(self, dim):\n",
    "        self.gamma = np.ones(dim)\n",
    "        self.beta = np.zeros(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / np.sqrt(var + 1e-8) + self.beta\n",
    "    \n",
    "class EncodagePosition:\n",
    "    def __init__(self, dim_modele, max_len=5000):\n",
    "        self.encodage = self.creer_encodage_position(max_len, dim_modele)\n",
    "        \n",
    "    def creer_encodage_position(self, max_len, dim_modele):\n",
    "        encodage = np.zeros((max_len, dim_modele))\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, dim_modele, 2) * -(np.log(10000.0) / dim_modele))\n",
    "        encodage[:, 0::2] = np.sin(position * div_term)\n",
    "        encodage[:, 1::2] = np.cos(position * div_term)\n",
    "        return encodage[np.newaxis, :, :]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.encodage[:, :x.shape[1], :]\n",
    "\n",
    "# Exemple d'utilisation\n",
    "taille_vocab = 1000\n",
    "dim_modele = 256\n",
    "nb_tetes = 8\n",
    "dim_ff = 512\n",
    "nb_couches = 6\n",
    "\n",
    "transformer = Transformer(taille_vocab, dim_modele, nb_tetes, dim_ff, nb_couches)\n",
    "\n",
    "# Séquence d'entrée (batch de 2 séquences de longueur 10)\n",
    "x = np.random.randint(0, taille_vocab, size=(2, 10))\n",
    "\n",
    "# Propagation avant\n",
    "sortie = transformer.forward(x)\n",
    "print(\"Forme de la sortie:\", sortie.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
